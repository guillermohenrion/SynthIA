{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##SinthIA: A Consensus-Based Dialogue System\n",
        "\n",
        "SinthIA is an advanced dialogue system designed to generate consensus-based responses by querying multiple AI models, including GPT-4 and Llama. It intelligently analyzes the responses from these models, synthesizes the best parts, and creates a final, unified response. This process ensures that the output is well-rounded and reflects a balanced perspective, making it ideal for applications in decision-making support, conflict resolution, and collaborative problem-solving.\n",
        "\n",
        "Key Features:\n",
        "Multiple AI Model Integration: Utilizes both GPT-4 and Llama for generating diverse responses.\n",
        "Consensus Building: Synthesizes responses from different models to create a consensus-based output.\n",
        "Customizable Iterations: Configurable process for refining responses through multiple iterations.\n",
        "Versatile Applications: Useful for decision-making tasks, advice generation, and conflict resolution."
      ],
      "metadata": {
        "id": "Et3QmcYr2v05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv openai==0.28\n",
        "\n",
        "import openai\n",
        "import requests\n",
        "from typing import Optional\n",
        "import json\n",
        "\n",
        "class SinthIA:\n",
        "    def __init__(self, config_file=\"config.json\", max_iterations=3):\n",
        "        # Cargar claves desde archivo JSON\n",
        "        self.config = self._load_config(config_file)\n",
        "        self.openai_api_key = self.config[\"openai_api_key\"]\n",
        "        self.llama_api_key = self.config[\"llama_api_key\"]\n",
        "        self.llama_endpoint = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "        # Configuraci√≥n de OpenAI\n",
        "        openai.api_key = self.openai_api_key\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_config(config_file: str) -> dict:\n",
        "        \"\"\"Carga las claves desde un archivo JSON.\"\"\"\n",
        "        try:\n",
        "            with open(config_file, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Configuration file not found: {config_file}\")\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(f\"The file {config_file} does not have a valid JSON format.\")\n",
        "\n",
        "    def query_gpt(self, prompt: str) -> str:\n",
        "        \"\"\"Consulta a GPT-4\"\"\"\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a neutral assistant focused on finding consensus.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message['content']\n",
        "        except Exception as e:\n",
        "            print(\"Error querying GPT.:\", e)\n",
        "            return \"Error getting response from GPT.\"\n",
        "\n",
        "    def query_llama(self, prompt: str) -> str:\n",
        "        \"\"\"Consulta a Llama 3\"\"\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.llama_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": \"llama3-70b-8192\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a neutral assistant focused on finding consensus..\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.llama_endpoint, headers=headers, json=payload)\n",
        "            response.raise_for_status()  # Verifica si hubo errores HTTP\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        except requests.RequestException as e:\n",
        "            print(\"Error querying Llama.:\", e)\n",
        "            return \"Error getting response from Llama.\"\n",
        "\n",
        "    def analyze_responses(self, gpt_response: str, llama_response: str) -> str:\n",
        "        \"\"\"Analyze and synthesize the responses.\"\"\"\n",
        "        mediation_prompt = f\"\"\"\n",
        "        You have two different responses:\n",
        "\n",
        "        GPT's response: {gpt_response}\n",
        "\n",
        "        Llama's response: {llama_response}\n",
        "\n",
        "        Analyze both responses. Identify the strengths of each and create\n",
        "        a consensus-based response that integrates the best aspects of both.\n",
        "        \"\"\"\n",
        "        return self.query_gpt(mediation_prompt)\n",
        "\n",
        "    def dialogue(self, initial_prompt: str) -> str:\n",
        "        \"\"\"\"Main dialogue process.\"\"\"\n",
        "        print(\"ü§ñ Starting SinthIA dialogue....\")\n",
        "\n",
        "        # Respuestas iniciales\n",
        "        gpt_response = self.query_gpt(initial_prompt)\n",
        "        llama_response = self.query_llama(initial_prompt)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            print(f\"\\n--- Iteraci√≥n {iteration + 1} ---\")\n",
        "            print(\"Respuesta GPT:\", gpt_response)\n",
        "            print(\"Respuesta Llama:\", llama_response)\n",
        "\n",
        "            # Encuentra consenso\n",
        "            consensus = self.analyze_responses(gpt_response, llama_response)\n",
        "            print(\"Consenso:\", consensus)\n",
        "\n",
        "            # Condici√≥n de parada\n",
        "            if self._consensus_reached(gpt_response, llama_response, consensus):\n",
        "                return consensus\n",
        "\n",
        "            # Actualiza las respuestas\n",
        "            gpt_response = self.query_gpt(consensus)\n",
        "            llama_response = self.query_llama(consensus)\n",
        "\n",
        "        return consensus\n",
        "\n",
        "    def _consensus_reached(self, gpt_resp: str, llama_resp: str, consensus: str) -> bool:\n",
        "        \"\"\"Basic consensus check.\"\"\"\n",
        "        return len(consensus) > len(gpt_resp) and len(consensus) > len(llama_resp)\n",
        "\n",
        "def main():\n",
        "    sinthia = SinthIA()\n",
        "    prompt = \"Recommendations for training in GenAI\"\n",
        "    resultado_final = sinthia.dialogue(prompt)\n",
        "    print(\"\\nüèÅ Resultado Final:\", resultado_final)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "y48WWZB1TnVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}